{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathi\\AppData\\Local\\Temp\\ipykernel_1924\\3389183221.py:1: DtypeWarning: Columns (46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(r'D:\\Project Phase II\\Dataset\\finaltrain.csv',encoding='cp1252')\n",
      "C:\\Users\\sathi\\AppData\\Local\\Temp\\ipykernel_1924\\3389183221.py:2: DtypeWarning: Columns (46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv(r'D:\\Project Phase II\\Dataset\\finaltest.csv',encoding='cp1252') #test_set1\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(r'D:\\Project Phase II\\Dataset\\finaltrain.csv',encoding='cp1252')\n",
    "test = pd.read_csv(r'D:\\Project Phase II\\Dataset\\finaltest.csv',encoding='cp1252') #test_set1 \n",
    "autest = pd.read_csv(r'D:\\Project Phase II\\Dataset\\UNSW_NB15_testing-set.csv',encoding='cp1252') #test_set2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess user-made train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "todrop = ['sloss', 'dloss','stcpb', 'dtcpb', 'trans_depth', 'Stime', 'Ltime','tcprtt', 'ct_flw_http_mthd', \n",
    "        'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ ltm',\n",
    "        'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat']\n",
    "        \n",
    "reducedTrain = train.drop(todrop, axis = 1)\n",
    "reducedTest = test.drop(todrop, axis = 1)\n",
    "reducedTrain = reducedTrain.drop_duplicates()\n",
    "reducedTest = reducedTest.drop_duplicates()\n",
    "\n",
    "trainAttributes = reducedTrain.drop(['srcip','sport','dstip','dsport','Label'], axis = 1)\n",
    "trainLabel = reducedTrain['Label']\n",
    "testAttributes = reducedTest.drop(['srcip','sport','dstip','dsport','Label'], axis = 1)\n",
    "testLabel = reducedTest['Label']\n",
    "\n",
    "train = reducedTrain\n",
    "test = reducedTest\n",
    "\n",
    "train = train.drop_duplicates(['srcip','sport', 'dstip', 'dsport', 'sttl', 'dttl', 'swin', 'dwin'], keep = 'last')\n",
    "test = test.drop_duplicates(['srcip','sport', 'dstip', 'dsport', 'sttl', 'dttl', 'swin', 'dwin'], keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      " Label\n",
      "0    39990\n",
      "1    39821\n",
      "Name: count, dtype: int64\n",
      "Test Set:\n",
      " Label\n",
      "1    17217\n",
      "0    17201\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set:\\n\",train.Label.value_counts())\n",
    "print(\"Test Set:\\n\", test.Label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Author's testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use stcpb, dtcpb to substitute the unavailablity of ip port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "autestdrop = ['ï»¿id', 'rate', 'sloss', 'dloss','trans_depth','tcprtt','ct_srv_src','ct_dst_ltm',\n",
    "        'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
    "        'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
    "        'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat']\n",
    "\n",
    "autest = autest.drop(autestdrop, axis = 1)\n",
    "#Change column names of autest\n",
    "autest.rename(columns={'dinpkt':'Dintpkt','djit':'Djit','dload':'Dload','dpkts':'Dpkts','label':'Label','sinpkt':'Sintpkt','sjit':'Sjit', 'sload':'Sload', 'spkts':'Spkts','dmean':'dmeansz','response_body_len':'res_bdy_len', 'smean':'smeansz'}, inplace=True)\n",
    "autest = autest[(autest['proto']=='tcp') | (autest['proto']=='udp') | (autest['proto']=='ospf')]\n",
    "autest = autest[(autest['service']=='ssh') | (autest['service']=='ftp-data ')  | (autest['service']=='ftp') | (autest['service']=='-') | (autest['service']=='dns') | (autest['service']=='smtp') | (autest['service']=='http') | (autest['service']=='radius') | (autest['service']=='pop3') ]\n",
    "autest = autest[(autest['state'] == 'CON') | (autest['state'] == 'RST') | (autest['state'] == 'FIN') | (autest['state'] == 'ACC') | (autest['state'] == 'REQ') | (autest['state'] == 'INT')]\n",
    "\n",
    "autest['proto'].replace(\"tcp\", 0, inplace = True)\n",
    "autest['proto'].replace(\"udp\", 1, inplace = True)\n",
    "autest['proto'].replace(\"ospf\", 2, inplace = True)\n",
    "\n",
    "autest['service'].replace(\"ssh\", 0, inplace = True)\n",
    "autest['service'].replace(\"ftp-data\", 1, inplace = True)\n",
    "autest['service'].replace(\"ftp\", 2, inplace = True)\n",
    "autest['service'].replace(\"-\", 3, inplace = True)\n",
    "autest['service'].replace(\"dns\", 4, inplace = True)\n",
    "autest['service'].replace(\"smtp\", 5, inplace = True)\n",
    "autest['service'].replace(\"http\", 6, inplace = True)\n",
    "autest['service'].replace(\"radius\", 7, inplace = True)\n",
    "autest['service'].replace(\"pop3\", 8, inplace = True)\n",
    "\n",
    "autest['state'].replace(\"CON\", 0, inplace = True)\n",
    "autest['state'].replace(\"RST\", 1, inplace = True)\n",
    "autest['state'].replace(\"FIN\", 2, inplace = True)\n",
    "autest['state'].replace(\"ACC\", 3, inplace = True)\n",
    "autest['state'].replace(\"REQ\", 4, inplace = True)\n",
    "autest['state'].replace(\"INT\", 5, inplace = True)\n",
    "\n",
    "for column in autest.columns:\n",
    "        if column != 'Label':\n",
    "                col_mean = sum(autest[column]) / len(autest[column])\n",
    "                col_std = (sum((x - col_mean) ** 2 for x in autest[column]) / len(autest[column])) ** 0.5\n",
    "                autest[column] = [(x - col_mean) / col_std for x in autest[column]]\n",
    "\n",
    "\n",
    "autest = autest.drop_duplicates(['sttl', 'dttl', 'swin', 'dwin','stcpb','dtcpb'], keep = 'last')\n",
    "autestAttributes = autest.drop(['Label'], axis = 1)\n",
    "autestAttributes = autestAttributes[trainAttributes.columns]\n",
    "autestLabel = autest['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author's Testing Set:\n",
      " Label\n",
      "0    24354\n",
      "1    13671\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Author's Testing Set:\\n\", autest.Label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set()\n",
    "trainUnique = train\n",
    "for i in range(len(train)):\n",
    "    src = str(train['srcip'].iloc[i])+':'+ str(train['sport'].iloc[i]) + ':' + str(train['sttl'].iloc[i])+':'+str(train['swin'].iloc[i])\n",
    "    dst = str(train['dstip'].iloc[i])+':'+str(train['dsport'].iloc[i]) + ':' + str(train['dttl'].iloc[i])+':'+str(train['dwin'].iloc[i])\n",
    "    nodes.add(src)\n",
    "    nodes.add(dst)\n",
    "\n",
    "\n",
    "#Find unique (srcip,sport,dstip,dsport) tuples in train\n",
    "train_tuples = set()\n",
    "for i in range(len(train)):\n",
    "    train_tuples.add((train['srcip'].iloc[i], train['sport'].iloc[i], train['dstip'].iloc[i], train['dsport'].iloc[i], train['sttl'].iloc[i], train['swin'].iloc[i], train['dttl'].iloc[i], train['dwin'].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97138, 3]) torch.Size([2, 79856]) torch.Size([79856, 20])\n"
     ]
    }
   ],
   "source": [
    "xLookUp = {}\n",
    "x = []\n",
    "count = 0\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "for i in range(len(train)):\n",
    "    src = str(train['srcip'].iloc[i])+':'+ str(train['sport'].iloc[i]) + ':' + str(train['sttl'].iloc[i])+':'+str(train['swin'].iloc[i])\n",
    "    dst = str(train['dstip'].iloc[i])+':'+str(train['dsport'].iloc[i]) + ':' + str(train['dttl'].iloc[i])+':'+str(train['dwin'].iloc[i])\n",
    "    if src not in xLookUp:\n",
    "        xLookUp[src] = count\n",
    "        x.append([int(train['sport'].iloc[i]), int(train['sttl'].iloc[i]), int(train['swin'].iloc[i])])\n",
    "        count += 1\n",
    "    if dst not in xLookUp:\n",
    "        xLookUp[dst] = count\n",
    "        x.append([int(train['dsport'].iloc[i]), int(train['dttl'].iloc[i]), int(train['dwin'].iloc[i])])\n",
    "        count += 1\n",
    "    edge_index.append([xLookUp[src], xLookUp[dst]])\n",
    "    edge_attr.append(list(train.iloc[i].drop(['srcip', 'sport', 'dstip', 'dsport', 'sttl', 'dttl','swin','dwin','Label']).values))\n",
    "\n",
    "\n",
    "# Convert x to tensor\n",
    "x_tensor = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "# Convert edge_index to tensor\n",
    "edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t()  # Transpose for correct format\n",
    "\n",
    "# Convert edge_attr to tensor\n",
    "edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "print(x_tensor.shape, edge_index_tensor.shape, edge_attr_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnodes = set()\n",
    "testUnique = test\n",
    "for i in range(len(test)):\n",
    "    src = str(test['srcip'].iloc[i])+':'+ str(test['sport'].iloc[i]) + ':' + str(test['sttl'].iloc[i])+':'+str(test['swin'].iloc[i])\n",
    "    dst = str(test['dstip'].iloc[i])+':'+str(test['dsport'].iloc[i]) + ':' + str(test['dttl'].iloc[i])+':'+str(test['dwin'].iloc[i])\n",
    "    testnodes.add(src)\n",
    "    testnodes.add(dst)\n",
    "\n",
    "\n",
    "#Find unique (srcip,sport,dstip,dsport) tuples in train\n",
    "test_tuples = set()\n",
    "for i in range(len(test)):\n",
    "    test_tuples.add((test['srcip'].iloc[i], test['sport'].iloc[i], test['dstip'].iloc[i], test['dsport'].iloc[i], test['sttl'].iloc[i], test['swin'].iloc[i], test['dttl'].iloc[i], test['dwin'].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42906, 3]) torch.Size([2, 34430]) torch.Size([34430, 20])\n"
     ]
    }
   ],
   "source": [
    "xtestLookUp = {}\n",
    "xtest = []\n",
    "count = 0\n",
    "edge_index_test = []\n",
    "edge_attr_test = []\n",
    "for i in range(len(test)):\n",
    "    src = str(test['srcip'].iloc[i])+':'+ str(test['sport'].iloc[i]) + ':' + str(test['sttl'].iloc[i])+':'+str(test['swin'].iloc[i])\n",
    "    dst = str(test['dstip'].iloc[i])+':'+str(test['dsport'].iloc[i]) + ':' + str(test['dttl'].iloc[i])+':'+str(test['dwin'].iloc[i])\n",
    "    if src not in xtestLookUp:\n",
    "        xtestLookUp[src] = count\n",
    "        xtest.append([int(test['sport'].iloc[i]), int(test['sttl'].iloc[i]), int(test['swin'].iloc[i])])\n",
    "        count += 1\n",
    "    if dst not in xtestLookUp:\n",
    "        xtestLookUp[dst] = count\n",
    "        xtest.append([int(test['dsport'].iloc[i]), int(test['dttl'].iloc[i]), int(test['dwin'].iloc[i])])\n",
    "        count += 1\n",
    "    edge_index_test.append([xtestLookUp[src], xtestLookUp[dst]])\n",
    "    edge_attr_test.append(list(test.iloc[i].drop(['srcip', 'sport', 'dstip', 'dsport', 'sttl', 'dttl','swin','dwin','Label']).values))\n",
    "\n",
    "\n",
    "# Convert x to tensor\n",
    "x_test_tensor = torch.tensor(xtest, dtype=torch.float)\n",
    "\n",
    "# Convert edge_index to tensor\n",
    "edge_index_test_tensor = torch.tensor(edge_index_test, dtype=torch.long).t()  # Transpose for correct format\n",
    "\n",
    "# Convert edge_attr to tensor\n",
    "edge_attr_test_tensor = torch.tensor(edge_attr_test, dtype=torch.float)\n",
    "\n",
    "print(x_test_tensor.shape, edge_index_test_tensor.shape, edge_attr_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author's Testing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since srcip, sport, dstip, dsport information is not available, we proceed with having each node feature value as 0, and assume each edge as an unique connection between two imaginary nodes, we add stcpb and dtcpb as placeholders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "autestnodes = set()\n",
    "autestUnique = autest\n",
    "for i in range(len(autest)):\n",
    "    src = str(autest['sttl'].iloc[i])+':'+str(autest['swin'].iloc[i]) + str(autest['stcpb'].iloc[i])\n",
    "    dst = str(autest['dttl'].iloc[i])+':'+str(autest['dwin'].iloc[i]) + str(autest['dtcpb'].iloc[i])\n",
    "    autestnodes.add(src)\n",
    "    autestnodes.add(dst)\n",
    "\n",
    "\n",
    "#Find unique (srcip,sport,dstip,dsport) tuples in train\n",
    "autest_tuples = set()\n",
    "for i in range(len(autest)):\n",
    "    autest_tuples.add((autest['sttl'].iloc[i], autest['swin'].iloc[i], autest['dttl'].iloc[i], autest['dwin'].iloc[i], autest['stcpb'].iloc[i], autest['dtcpb'].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75920, 3]) torch.Size([2, 38025]) torch.Size([38025, 20])\n"
     ]
    }
   ],
   "source": [
    "xautestLookUp = {}\n",
    "xautest = []\n",
    "count = 0\n",
    "edge_index_autest = []\n",
    "edge_attr_autest = []\n",
    "\n",
    "for i in range(len(autest)):\n",
    "    src = str(autest['sttl'].iloc[i])+':'+str(autest['swin'].iloc[i]) + str(autest['stcpb'].iloc[i])\n",
    "    dst = str(autest['dttl'].iloc[i])+':'+str(autest['dwin'].iloc[i]) + str(autest['dtcpb'].iloc[i])\n",
    "    if src not in xautestLookUp:\n",
    "        xautestLookUp[src] = count\n",
    "        xautest.append([0,int(autest['sttl'].iloc[i]), int(autest['swin'].iloc[i])])\n",
    "        count += 1\n",
    "    if dst not in xautestLookUp:\n",
    "        xautestLookUp[dst] = count\n",
    "        xautest.append([0, int(autest['dttl'].iloc[i]), int(autest['dwin'].iloc[i])])\n",
    "        count += 1\n",
    "    edge_index_autest.append([xautestLookUp[src], xautestLookUp[dst]])\n",
    "    edge_attr_autest.append(list(autest.iloc[i].drop(['sttl', 'dttl','swin','dwin','Label','stcpb','dtcpb']).values))\n",
    "\n",
    "\n",
    "\n",
    "# Convert x to tensor\n",
    "x_autest_tensor = torch.tensor(xautest, dtype=torch.float)\n",
    "\n",
    "# Convert edge_index to tensor\n",
    "edge_index_autest_tensor = torch.tensor(edge_index_autest, dtype=torch.long).t()  # Transpose for correct format\n",
    "\n",
    "# Convert edge_attr to tensor\n",
    "edge_attr_autest_tensor = torch.tensor(edge_attr_autest, dtype=torch.float)\n",
    "\n",
    "print(x_autest_tensor.shape, edge_index_autest_tensor.shape, edge_attr_autest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear appplies linear transformation to the incoming data: y = x * transpose(A) + b \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GATClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_channels, num_heads):\n",
    "        super(GATClassifier, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels, heads=num_heads, add_self_loops=False)\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, add_self_loops=False)\n",
    "        # self.edge_classifier = nn.Linear(hidden_channels * num_heads + num_edge_features, 1)  # Output size changed to 1\n",
    "        self.edge_classifier = nn.Linear(hidden_channels * 2 * num_heads + num_edge_features, 1)  # Output size changed to 1\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Apply graph attentional layers\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Concatenate node features with edge features\n",
    "        num_edges = edge_index.size(1)\n",
    "        node_x = x[edge_index[0]]\n",
    "        node_x_other = x[edge_index[1]]\n",
    "        edge_attr_padded = torch.cat([edge_attr, torch.zeros(num_edges - edge_attr.size(0), edge_attr.size(1))], dim=0)\n",
    "        edge_x = torch.cat([node_x, node_x_other, edge_attr_padded], dim=1)\n",
    "        print(\"edge_x.shape:\", edge_x.shape)\n",
    "        print(\"self.edge_classifier.weight.shape:\",self.edge_classifier.weight.shape)\n",
    "        # Classify edges into binary class using BCELogit\n",
    "        edge_scores = self.edge_classifier(edge_x)\n",
    "        return edge_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_x.shape: torch.Size([79856, 148])\n",
      "self.edge_classifier.weight.shape: torch.Size([1, 148])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_nodes = len(nodes)\n",
    "num_edges = len(train_tuples)\n",
    "num_node_features = 3\n",
    "num_edge_features = 20\n",
    "hidden_channels = 16\n",
    "num_heads = 4\n",
    "\n",
    "# Instantiate GAT classifier\n",
    "model = GATClassifier(num_node_features, num_edge_features, hidden_channels, num_heads)\n",
    "\n",
    "# Forward pass\n",
    "edge_scores_train = model(x_tensor, edge_index_tensor, edge_attr_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_x.shape: torch.Size([34430, 148])\n",
      "self.edge_classifier.weight.shape: torch.Size([1, 148])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_nodes_test = len(testnodes)\n",
    "num_edges_test = len(test_tuples)\n",
    "num_node_features = 3\n",
    "num_edge_features = 20\n",
    "hidden_channels = 4\n",
    "num_heads = 16\n",
    "\n",
    "# Forward pass\n",
    "edge_scores_test = model(x_test_tensor, edge_index_test_tensor, edge_attr_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author's testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_x.shape: torch.Size([38025, 148])\n",
      "self.edge_classifier.weight.shape: torch.Size([1, 148])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_nodes_test = len(autestnodes)\n",
    "num_edges_test = len(autest_tuples)\n",
    "num_node_features = 3\n",
    "num_edge_features = 20\n",
    "hidden_channels = 16\n",
    "num_heads = 4\n",
    "\n",
    "# Forward pass\n",
    "edge_scores_autest = model(x_autest_tensor, edge_index_autest_tensor, edge_attr_autest_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Convert edge_scores tensor to a NumPy array\n",
    "edge_scores_train_array = edge_scores_train.detach().numpy()\n",
    "edge_scores_test_array = edge_scores_test.detach().numpy()\n",
    "# Create a DataFrame from the NumPy array\n",
    "edge_scores_train_df = pd.DataFrame(edge_scores_train_array, columns=['Edge_Scores'])\n",
    "edge_scores_test_df = pd.DataFrame(edge_scores_test_array, columns=['Edge_Scores'])\n",
    "trainLabel = train['Label']\n",
    "testLabel = test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90     17201\n",
      "           1       0.92      0.86      0.89     17229\n",
      "\n",
      "    accuracy                           0.89     34430\n",
      "   macro avg       0.90      0.89      0.89     34430\n",
      "weighted avg       0.90      0.89      0.89     34430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train = edge_scores_train_df\n",
    "X_test = edge_scores_test_df\n",
    "y_train = trainLabel\n",
    "y_test = testLabel\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "k = 10  # Number of neighbors\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the classifier\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report: \\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author's test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.64      0.68     24354\n",
      "           1       0.48      0.58      0.52     13671\n",
      "\n",
      "    accuracy                           0.62     38025\n",
      "   macro avg       0.60      0.61      0.60     38025\n",
      "weighted avg       0.64      0.62      0.63     38025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convert edge_scores tensor to a NumPy array\n",
    "edge_scores_autest_array = edge_scores_autest.detach().numpy()\n",
    "# Create a DataFrame from the NumPy array\n",
    "testLabeldf = pd.DataFrame(auEdgeLabel, columns=['Label'])\n",
    "edge_scores_autest_df = pd.DataFrame(edge_scores_autest_array, columns=['Edge_Scores'])\n",
    "trainLabel = train['Label']\n",
    "autestLabel = autest['Label']\n",
    "auX_test = edge_scores_autest_df\n",
    "auy_test = autestLabel \n",
    "\n",
    "auy_pred = knn_classifier.predict(edge_scores_autest_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "report = classification_report(autestLabel, auy_pred)\n",
    "print(\"Classification Report: \\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Predict Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 11.0.0. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tuple' object has no attribute '__name__'\n",
      "Invalid Classifier(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:01<00:09,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'AdaBoostClassifier', 'Accuracy': 0.886494336334592, 'Balanced Accuracy': 0.8865193459587084, 'ROC AUC': 0.8865193459587084, 'F1 Score': 0.8863890964369112, 'Time taken': 1.9350829124450684}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:04<00:08,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'BaggingClassifier', 'Accuracy': 0.8571304095265757, 'Balanced Accuracy': 0.8571403063981533, 'ROC AUC': 0.8571403063981532, 'F1 Score': 0.857110269484069, 'Time taken': 2.138676166534424}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:04<00:03,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'DecisionTreeClassifier', 'Accuracy': 0.8525994772001162, 'Balanced Accuracy': 0.8526028518893403, 'ROC AUC': 0.8526028518893403, 'F1 Score': 0.852597302023816, 'Time taken': 0.268186092376709}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:24<00:17,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'RandomForestClassifier', 'Accuracy': 0.8527446993900668, 'Balanced Accuracy': 0.8527477671122392, 'ROC AUC': 0.8527477671122391, 'F1 Score': 0.8527429349306606, 'Time taken': 20.119638681411743}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:24<00:05,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'XGBClassifier', 'Accuracy': 0.8928550682544293, 'Balanced Accuracy': 0.8928748620126774, 'ROC AUC': 0.8928748620126774, 'F1 Score': 0.8927932343956324, 'Time taken': 0.343428373336792}\n",
      "[LightGBM] [Info] Number of positive: 39866, number of negative: 39990\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 79856, number of used features: 1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499224 -> initscore=-0.003106\n",
      "[LightGBM] [Info] Start training from score -0.003106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:25<00:00,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LGBMClassifier', 'Accuracy': 0.8938135347081034, 'Balanced Accuracy': 0.8938354785419262, 'ROC AUC': 0.8938354785419262, 'F1 Score': 0.893738011755283, 'Time taken': 0.5980427265167236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of classifiers to include\n",
    "import sklearn\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import sklearn.discriminant_analysis\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    ('AdaBoostClassifier', sklearn.ensemble._weight_boosting.AdaBoostClassifier),\n",
    "    ('BaggingClassifier', sklearn.ensemble._bagging.BaggingClassifier),\n",
    "    ('DecisionTreeClassifier', sklearn.tree._classes.DecisionTreeClassifier),\n",
    "    # ('ExtraTreeClassifier', sklearn.ensemble._forest.ExtraTreeClassifier),\n",
    "    # ('ExtraTreesClassifier', sklearn.ensemble._forest.ExtraTreesClassifier),\n",
    "    # ('KNeighborsClassifier',  sklearn.neighbors._classification.KNeighborsClassifier),\n",
    "    # ('NuSVC', sklearn.svm._classes.NuSVC),\n",
    "    ('RandomForestClassifier', sklearn.ensemble._forest.RandomForestClassifier),\n",
    "    # ('SVC', sklearn.svm._classes.SVC),\n",
    "    ('XGBClassifier', xgboost.sklearn.XGBClassifier),\n",
    "    ('LGBMClassifier', lightgbm.sklearn.LGBMClassifier)]\n",
    "clf = LazyClassifier(verbose=1,ignore_warnings=True, custom_metric=None,classifiers= classifiers, predictions=True)\n",
    "\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89     17201\n",
      "           1       0.91      0.86      0.88     17229\n",
      "\n",
      "    accuracy                           0.89     34430\n",
      "   macro avg       0.89      0.89      0.89     34430\n",
      "weighted avg       0.89      0.89      0.89     34430\n",
      "\n",
      "AUC Scores for each class: 0.8865193459587084\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86     17201\n",
      "           1       0.87      0.84      0.86     17229\n",
      "\n",
      "    accuracy                           0.86     34430\n",
      "   macro avg       0.86      0.86      0.86     34430\n",
      "weighted avg       0.86      0.86      0.86     34430\n",
      "\n",
      "AUC Scores for each class: 0.8571403063981532\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.85     17201\n",
      "           1       0.86      0.85      0.85     17229\n",
      "\n",
      "    accuracy                           0.85     34430\n",
      "   macro avg       0.85      0.85      0.85     34430\n",
      "weighted avg       0.85      0.85      0.85     34430\n",
      "\n",
      "AUC Scores for each class: 0.8526028518893403\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.85     17201\n",
      "           1       0.86      0.85      0.85     17229\n",
      "\n",
      "    accuracy                           0.85     34430\n",
      "   macro avg       0.85      0.85      0.85     34430\n",
      "weighted avg       0.85      0.85      0.85     34430\n",
      "\n",
      "AUC Scores for each class: 0.8527477671122391\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.90     17201\n",
      "           1       0.91      0.87      0.89     17229\n",
      "\n",
      "    accuracy                           0.89     34430\n",
      "   macro avg       0.89      0.89      0.89     34430\n",
      "weighted avg       0.89      0.89      0.89     34430\n",
      "\n",
      "AUC Scores for each class: 0.8928748620126774\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "LGBMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.90     17201\n",
      "           1       0.92      0.87      0.89     17229\n",
      "\n",
      "    accuracy                           0.89     34430\n",
      "   macro avg       0.89      0.89      0.89     34430\n",
      "weighted avg       0.89      0.89      0.89     34430\n",
      "\n",
      "AUC Scores for each class: 0.8938354785419262\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "for m in predictions.columns:\n",
    "    print(m)\n",
    "    print(classification_report(y_test, predictions[m]))\n",
    "    auc_scores = roc_auc_score(y_test, predictions[m], multi_class='ovr')  # Or multi_class='ovo' for one-vs-one\n",
    "    print(\"AUC Scores for each class:\", auc_scores)\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy Predict for author's testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tuple' object has no attribute '__name__'\n",
      "Invalid Classifier(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:02<00:11,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'AdaBoostClassifier', 'Accuracy': 0.6285075608152532, 'Balanced Accuracy': 0.6168067796144676, 'ROC AUC': 0.6168067796144676, 'F1 Score': 0.6340335316967551, 'Time taken': 2.3837015628814697}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:04<00:08,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'BaggingClassifier', 'Accuracy': 0.591163708086785, 'Balanced Accuracy': 0.5980655341394917, 'ROC AUC': 0.5980655341394916, 'F1 Score': 0.5993969804128663, 'Time taken': 2.0966784954071045}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:04<00:03,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'DecisionTreeClassifier', 'Accuracy': 0.5834319526627219, 'Balanced Accuracy': 0.5940028857866331, 'ROC AUC': 0.5940028857866331, 'F1 Score': 0.5917383654112943, 'Time taken': 0.2701249122619629}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:24<00:17,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'RandomForestClassifier', 'Accuracy': 0.5835634451019066, 'Balanced Accuracy': 0.5941697113721391, 'ROC AUC': 0.5941697113721393, 'F1 Score': 0.5918663115286102, 'Time taken': 20.169023990631104}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:25<00:05,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'XGBClassifier', 'Accuracy': 0.6235108481262327, 'Balanced Accuracy': 0.6095529414906733, 'ROC AUC': 0.6095529414906733, 'F1 Score': 0.6287588517430759, 'Time taken': 0.28249549865722656}\n",
      "[LightGBM] [Info] Number of positive: 39866, number of negative: 39990\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 79856, number of used features: 1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499224 -> initscore=-0.003106\n",
      "[LightGBM] [Info] Start training from score -0.003106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:25<00:00,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LGBMClassifier', 'Accuracy': 0.6260092044707429, 'Balanced Accuracy': 0.6112466476072186, 'ROC AUC': 0.6112466476072185, 'F1 Score': 0.6310145015398337, 'Time taken': 0.3919703960418701}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of classifiers to include\n",
    "import sklearn\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import sklearn.discriminant_analysis\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "classifiers = [\n",
    "    ('AdaBoostClassifier', sklearn.ensemble._weight_boosting.AdaBoostClassifier),\n",
    "    ('BaggingClassifier', sklearn.ensemble._bagging.BaggingClassifier),\n",
    "    ('DecisionTreeClassifier', sklearn.tree._classes.DecisionTreeClassifier),\n",
    "    # ('ExtraTreeClassifier', sklearn.ensemble._forest.ExtraTreeClassifier),\n",
    "    # ('ExtraTreesClassifier', sklearn.ensemble._forest.ExtraTreesClassifier),\n",
    "    # ('KNeighborsClassifier',  sklearn.neighbors._classification.KNeighborsClassifier),\n",
    "    # ('NuSVC', sklearn.svm._classes.NuSVC),\n",
    "    ('RandomForestClassifier', sklearn.ensemble._forest.RandomForestClassifier),\n",
    "    # ('SVC', sklearn.svm._classes.SVC),\n",
    "    ('XGBClassifier', xgboost.sklearn.XGBClassifier),\n",
    "    ('LGBMClassifier', lightgbm.sklearn.LGBMClassifier)]\n",
    "clf = LazyClassifier(verbose=1,ignore_warnings=True, custom_metric=None,classifiers= classifiers, predictions=True)\n",
    "\n",
    "aumodels,aupredictions = clf.fit(X_train, auX_test, y_train, auy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.66      0.69     24354\n",
      "           1       0.49      0.58      0.53     13671\n",
      "\n",
      "    accuracy                           0.63     38025\n",
      "   macro avg       0.61      0.62      0.61     38025\n",
      "weighted avg       0.64      0.63      0.63     38025\n",
      "\n",
      "AUC Scores for each class: 0.6168067796144676\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.57      0.64     24354\n",
      "           1       0.45      0.62      0.52     13671\n",
      "\n",
      "    accuracy                           0.59     38025\n",
      "   macro avg       0.59      0.60      0.58     38025\n",
      "weighted avg       0.63      0.59      0.60     38025\n",
      "\n",
      "AUC Scores for each class: 0.5980655341394916\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.56      0.63     24354\n",
      "           1       0.44      0.63      0.52     13671\n",
      "\n",
      "    accuracy                           0.58     38025\n",
      "   macro avg       0.59      0.59      0.58     38025\n",
      "weighted avg       0.63      0.58      0.59     38025\n",
      "\n",
      "AUC Scores for each class: 0.5940028857866331\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.56      0.63     24354\n",
      "           1       0.44      0.63      0.52     13671\n",
      "\n",
      "    accuracy                           0.58     38025\n",
      "   macro avg       0.59      0.59      0.58     38025\n",
      "weighted avg       0.63      0.58      0.59     38025\n",
      "\n",
      "AUC Scores for each class: 0.5941697113721393\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.66      0.69     24354\n",
      "           1       0.48      0.56      0.52     13671\n",
      "\n",
      "    accuracy                           0.62     38025\n",
      "   macro avg       0.60      0.61      0.60     38025\n",
      "weighted avg       0.64      0.62      0.63     38025\n",
      "\n",
      "AUC Scores for each class: 0.6095529414906733\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "LGBMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.66      0.69     24354\n",
      "           1       0.48      0.56      0.52     13671\n",
      "\n",
      "    accuracy                           0.63     38025\n",
      "   macro avg       0.61      0.61      0.61     38025\n",
      "weighted avg       0.64      0.63      0.63     38025\n",
      "\n",
      "AUC Scores for each class: 0.6112466476072185\n",
      "-----------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "for model in aupredictions.columns:\n",
    "    print(model)\n",
    "    print(classification_report(auy_test, aupredictions[model]))\n",
    "    auc_scores = roc_auc_score(auy_test, aupredictions[model], multi_class='ovr')  # Or multi_class='ovo' for one-vs-one\n",
    "    print(\"AUC Scores for each class:\", auc_scores)\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
