{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "todrop = ['srcip', 'sport', 'dstip', 'dsport','sloss', 'dloss','stcpb', 'dtcpb', 'trans_depth', 'Stime', 'Ltime','ct_flw_http_mthd', \n",
    "        'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ ltm',\n",
    "        'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['proto', 'state', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'service', 'Sload', 'Dload',\n",
    "       'Spkts', 'Dpkts', 'swin', 'dwin', 'smeansz',\n",
    "       'dmeansz',  'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
    "       'ct_state_ttl','Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathi\\AppData\\Local\\Temp\\ipykernel_16964\\2025491202.py:1: DtypeWarning: Columns (46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(r'D:\\Project Phase II\\Dataset\\finaltrain.csv',encoding='cp1252')\n",
      "C:\\Users\\sathi\\AppData\\Local\\Temp\\ipykernel_16964\\2025491202.py:2: DtypeWarning: Columns (46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv(r'D:\\Project Phase II\\Dataset\\finaltest.csv',encoding='cp1252')\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(r'D:\\Project Phase II\\Dataset\\finaltrain.csv',encoding='cp1252')\n",
    "test = pd.read_csv(r'D:\\Project Phase II\\Dataset\\finaltest.csv',encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Attack:  40502  Non attack:  40502 Total:  81004\n",
      "Testing set:\n",
      "Attack:  17358  Non attack:  17358 Total:  34716\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print('Attack: ', len(train[train['Label'] == 0]), ' Non attack: ', len(train[train['Label'] == 0]), 'Total: ', len(train))\n",
    "print(\"Testing set:\")\n",
    "print('Attack: ', len(test[test['Label'] == 0]), ' Non attack: ', len(test[test['Label'] == 0]), 'Total: ', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract edge features alone from train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "todrop = ['srcip', 'sport', 'dstip', 'dsport','sloss', 'dloss','stcpb', 'dtcpb', 'trans_depth', 'Stime', 'Ltime','ct_flw_http_mthd', \n",
    "        'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ ltm',\n",
    "        'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'Label']\n",
    "\n",
    "trainAttributes = train.drop(todrop, axis = 1)\n",
    "testAttributes = test.drop(todrop, axis = 1)\n",
    "\n",
    "trainLabel = train['Label']\n",
    "testLabel = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LDA to reduce dimension to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components = 1)\n",
    "lda_x_train = lda.fit_transform(trainAttributes, trainLabel)\n",
    "lda_x_test = lda.transform(testAttributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute nodes and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider only 7000 train records since memory becomes high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train.head(3500), train.tail(3500)])\n",
    "lda_x_train = np.concatenate((lda_x_train[:3500], lda_x_train[len(lda_x_train)-3500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set()\n",
    "edges = set()\n",
    "for i in range(7000):\n",
    "    src = str(train['srcip'].iloc[i])+':'+str(train['sport'].iloc[i])\n",
    "    dst = str(train['dstip'].iloc[i])+':'+str(train['dsport'].iloc[i])\n",
    "    nodes.add(src)\n",
    "    nodes.add(dst)\n",
    "    edges.add((src,dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8963"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathi\\AppData\\Local\\Temp\\ipykernel_16964\\3664507687.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  adjacency_matrix[src_index, dst_index] = lda_x_train[i]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Prepare an adjacency matrix of the nodes\n",
    "adjacency_matrix = np.zeros((len(nodes), len(nodes)), dtype=float)\n",
    "label_matrix = np.zeros((len(nodes), len(nodes)))\n",
    "\n",
    "\n",
    "# Add the columns srcip:sport and dstip:dsport to the adjacency matrix\n",
    "for i in range(7000):\n",
    "    src = str(train['srcip'].iloc[i])+':'+str(train['sport'].iloc[i])\n",
    "    dst = str(train['dstip'].iloc[i])+':'+str(train['dsport'].iloc[i])\n",
    "    src_index = list(nodes).index(src)\n",
    "    dst_index = list(nodes).index(dst)\n",
    "    adjacency_matrix[src_index, dst_index] = lda_x_train[i]\n",
    "    label_matrix[src_index, dst_index] = train['Label'].iloc[i]\n",
    "# Flatten tuple values in the adjacency matrix\n",
    "adjacency_matrix_flat = adjacency_matrix.reshape(-1, 1)\n",
    "label_flat = label_matrix.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider only 5000 records for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test.head(2500), test.tail(2500)])\n",
    "lda_x_test = np.concatenate((lda_x_test[:2500], lda_x_test[len(lda_x_test)-2500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenodes = set()\n",
    "teedges = set()\n",
    "for i in range(len(test)):\n",
    "    src = str(test['srcip'].iloc[i])+':'+str(test['sport'].iloc[i])\n",
    "    dst = str(test['dstip'].iloc[i])+':'+str(test['dsport'].iloc[i])\n",
    "    tenodes.add(src)\n",
    "    tenodes.add(dst)\n",
    "    teedges.add((src,dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathi\\AppData\\Local\\Temp\\ipykernel_16964\\4001184087.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  adjacency_matrix_test[src_index, dst_index] = lda_x_test[i]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Prepare an adjacency matrix of the nodes\n",
    "adjacency_matrix_test = np.zeros((len(tenodes), len(tenodes)), dtype=float)\n",
    "label_matrix_test = np.zeros((len(tenodes), len(tenodes)))\n",
    "\n",
    "\n",
    "# Add the columns srcip:sport and dstip:dsport to the adjacency matrix\n",
    "for i in range(len(test)):\n",
    "    src = str(test['srcip'].iloc[i])+':'+str(test['sport'].iloc[i])\n",
    "    dst = str(test['dstip'].iloc[i])+':'+str(test['dsport'].iloc[i])\n",
    "    src_index = list(tenodes).index(src)\n",
    "    dst_index = list(tenodes).index(dst)\n",
    "    adjacency_matrix_test[src_index, dst_index] = lda_x_test[i]\n",
    "    label_matrix_test[src_index, dst_index] = test['Label'].iloc[i]\n",
    "# Flatten tuple values in the adjacency matrix\n",
    "adjacency_matrix_test_flat = adjacency_matrix_test.reshape(-1, 1)\n",
    "label_test_flat = label_matrix_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\sathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2008385/2008385 [==============================] - 6828s 3ms/step - loss: 1.0093e-04 - accuracy: 1.0000 - val_loss: 1.5123e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "2008385/2008385 [==============================] - 6704s 3ms/step - loss: 1.3081e-05 - accuracy: 1.0000 - val_loss: 1.3967e-05 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "# Hyperparameters\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Modify the call method of the LSTMClassifier\n",
    "class LSTMClassifier(tf.keras.Model):\n",
    "    def __init__(self, input_shape, hidden_units, output_units):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = LSTM(hidden_units, return_sequences=False)  # Set return_sequences=False since we only need the output at the last timestep\n",
    "        self.dense = Dense(output_units, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.expand_dims(inputs, axis=-1)  # Add a new dimension to represent the input features\n",
    "        x = self.lstm(x)\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = adjacency_matrix_flat.shape[1:]  # Assuming each tuple has 2 attributes\n",
    "hidden_units = 16\n",
    "output_units = 1  # Binary classification\n",
    "\n",
    "# Convert adjacency_matrix_flat to a TensorFlow tensor\n",
    "adjacency_tensor = tf.convert_to_tensor(adjacency_matrix_flat, dtype=tf.float32)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "lstmmodel = LSTMClassifier(input_shape, hidden_units, output_units)\n",
    "lstmmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "lstmhistory = lstmmodel.fit(adjacency_tensor, label_flat, epochs=2, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmmodel.save_weights(\"lstmfinalldaweights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\sathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6931 - accuracy: 1.0000 - val_loss: 0.6921 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6921 - accuracy: 1.0000 - val_loss: 0.6911 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "# Hyperparameters\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Modify the call method of the LSTMClassifier\n",
    "class LSTMClassifier(tf.keras.Model):\n",
    "    def __init__(self, input_shape, hidden_units, output_units):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = LSTM(hidden_units, return_sequences=False)  # Set return_sequences=False since we only need the output at the last timestep\n",
    "        self.dense = Dense(output_units, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.expand_dims(inputs, axis=-1)  # Add a new dimension to represent the input features\n",
    "        x = self.lstm(x)\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "    \n",
    "input_shape = adjacency_matrix_flat.shape[1:]  # Assuming each tuple has 2 attributes\n",
    "hidden_units = 16\n",
    "output_units = 1  # Binary classification\n",
    "# Initialize model, optimizer, and loss function\n",
    "lstmmodel = LSTMClassifier(input_shape, hidden_units, output_units)\n",
    "\n",
    "dummy_input = np.zeros((4, 4), dtype=float)\n",
    "dummy_label = np.zeros((4, 4))\n",
    "#Use dummy data to initialize the model\n",
    "dummy_flat = dummy_input.reshape(-1, 1)\n",
    "dummy_label_flat = dummy_label.reshape(-1, 1) \n",
    "\n",
    "lstmmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstmhistory = lstmmodel.fit(dummy_flat, dummy_label_flat, epochs=2, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved weights\n",
    "lstmmodel.load_weights(\"lstmfinalldaweights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  16740/1350547 [..............................] - ETA: 41:28"
     ]
    }
   ],
   "source": [
    "lstmpredictions = lstmmodel.predict(adjacency_matrix_test_flat)\n",
    "lstmpredictions = (lstmpredictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(label_test_flat, lstmpredictions)\n",
    "\n",
    "print(\"Classification Report for LSTM:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "# Modify the call method of the GRUClassifier\n",
    "class GRUClassifier(tf.keras.Model):\n",
    "    def __init__(self, input_shape, hidden_units, output_units):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.gru = GRU(hidden_units, return_sequences=False)  # Set return_sequences=False since we only need the output at the last timestep\n",
    "        self.dense = Dense(output_units, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.expand_dims(inputs, axis=-1)  # Add a new dimension to represent the input features\n",
    "        x = self.gru(x)\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = adjacency_matrix_flat.shape[1:]  # Assuming each tuple has 2 attributes\n",
    "hidden_units = 16\n",
    "output_units = 1  # Binary classification\n",
    "\n",
    "grumodel = GRUClassifier(input_shape, hidden_units, output_units)\n",
    "\n",
    "\n",
    "grumodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gruhistory = grumodel.fit(adjacency_tensor, label_flat, epochs=2, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Convert adjacency_matrix_flat to a TensorFlow tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grumodel.save_weights(\"grufinalldaweights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grupredictions = grumodel.predict(adjacency_matrix_test_flat)\n",
    "grupredictions = (grupredictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(label_test_flat, grupredictions)\n",
    "\n",
    "print(\"Classification Report for GRU:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
